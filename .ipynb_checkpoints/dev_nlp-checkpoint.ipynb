{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing\n",
    "\n",
    "For each category (i.e. each sheet in the `tasks.xlsx` file):\n",
    "\n",
    "1. Create the document-term matrix `A` (called `vectors` below)\n",
    "\n",
    "\n",
    "2. Apply SVD (singular value decomposition) to decompose `AT` (term-document matrix) into:\n",
    "\n",
    "    * term-concept matrix `U`\n",
    "    * singular value matrix `sigma`\n",
    "    * concept-document matrix `V` in the form: `A = U*sigma*VT`\n",
    "        * want document-concept matrix `VT` for query mapping\n",
    "\n",
    "3. query mapping done in `runNLP.ipynb`\n",
    "\n",
    "\n",
    "The parameters: `U`, `sigma`, `VT`, `vocab` (or terms) are stored as NumPy matrices, and saved in compressed NumPy format `.npz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "file_name = \"tasks.xlsx\"\n",
    "npzfiles = ['nlp_1','nlp_2','nlp_3','nlp_4','nlp_5','nlp_6','nlp_7',\n",
    "            'nlp_8','nlp_9','nlp_10','nlp_11','nlp_12','nlp_13']\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "for sheet,npzfile in enumerate(npzfiles):\n",
    "    \n",
    "    text = [w[0] for w in pd.read_excel(file_name, sheet_name=sheet, header=None).values]\n",
    "    vectors = vectorizer.fit_transform(text).todense() # (documents, vocab)\n",
    "    U, s, VT = linalg.svd(vectors.T, full_matrices=False)\n",
    "    sigma = np.diag(s)\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    \n",
    "    np.savez('nlp_resources/'+npzfile+'.npz',U=U,sigma=sigma,VT=VT,vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Tasks\n",
    "\n",
    "The tasks are parsed from the `tasks.xlsx` file and saved in separate `JSON` files - one per category.\n",
    "\n",
    "\n",
    "Example (category 2, `tasks_2.json`):\n",
    "\n",
    ">```\n",
    "{\n",
    "\t\"0\": \"Did you know: It's better to challenge a fear food...\",\n",
    "\t\"1\": \"Did you know: When you eat low-carbs during the day...\",\n",
    "\t\"2\": \"Did you know: Asians eat lots of rice and carbs...\",\n",
    "\t\"3\": \"Did you know: If you start eating when you're not hungry...\",\n",
    "\t\"4\": \"Did you know: If you don't have enough carbs...\",\n",
    "\t\"5\": \"Did you know: Fibre helps digestion and is necessary...\",\n",
    "\t\"6\": \"Did you know: It is recommended that people consume fish...\",\n",
    "\t\"7\": \"Did you know: Many people consume far more protein than they need...\"\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfiles = ['tasks_1','tasks_2','tasks_3','tasks_4','tasks_5','tasks_6','tasks_7',\n",
    "             'tasks_8','tasks_9','tasks_10','tasks_11','tasks_12','tasks_13']\n",
    "\n",
    "for sheet,jsonfile in enumerate(jsonfiles):\n",
    "    \n",
    "    text = [w[0] for w in pd.read_excel(file_name, sheet_name=sheet, header=None).values]\n",
    "    dictOfTasks = { i : text[i] for i in range(0, len(text) ) }\n",
    "    with open('tasks/'+jsonfile+'.json', 'w') as fp:\n",
    "        json.dump(dictOfTasks, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "`targets (13 x 5)`: (category by task) - tagged top 5 tasks within each category\n",
    "\n",
    "\n",
    "**Queries** (which will be journal entries) emulated by web-scraped forum posts\n",
    "* stored in `journal_entries.xlsx`\n",
    "* one query per category\n",
    "\n",
    "\n",
    "Calculated accracy for each **dimension** (dimensionality reduction during SVD)\n",
    "* Optimal solution: 6 dimensions\n",
    "\n",
    "\n",
    "---\n",
    "* **accuracy by category** - _at least one_ of five tasks hit: 12/13 = 92.3%\n",
    "* **accuracy by task** - consider _all_ tagged tasks: 40/65 = 61.5%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.runNLP import nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.asarray(\n",
    "    [[44,51,52,12,25],\n",
    "     [1,2,4,7,6],\n",
    "     [31,43,18,42,14],\n",
    "     [21,24,11,17,22],\n",
    "     [0,2,8,9,25],\n",
    "     [25,20,23,27,21],\n",
    "     [2,5,14,12,18],\n",
    "     [0,1,15,16,3],\n",
    "     [5,4,3,21,23],\n",
    "     [6,27,34,2,39],\n",
    "     [13,11,12,18,26],\n",
    "     [9,11,0,22,23],\n",
    "     [11,8,22,9,20]]\n",
    ")\n",
    "\n",
    "entries = [w[0] for w in pd.read_excel('journal_entries.xlsx', header=None).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipynb.fs.full.runNLP:59: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions:\t1\n",
      "At least one:\t16.923076923076923\n",
      "All:\t\t46.15384615384615\n",
      "\n",
      "Dimensions:\t2\n",
      "At least one:\t26.153846153846157\n",
      "All:\t\t61.53846153846154\n",
      "\n",
      "Dimensions:\t3\n",
      "At least one:\t29.230769230769234\n",
      "All:\t\t69.23076923076923\n",
      "\n",
      "Dimensions:\t4\n",
      "At least one:\t38.46153846153847\n",
      "All:\t\t76.92307692307693\n",
      "\n",
      "Dimensions:\t5\n",
      "At least one:\t46.15384615384615\n",
      "All:\t\t76.92307692307693\n",
      "\n",
      "Dimensions:\t6\n",
      "At least one:\t61.53846153846154\n",
      "All:\t\t92.3076923076923\n",
      "\n",
      "Dimensions:\t7\n",
      "At least one:\t47.69230769230769\n",
      "All:\t\t92.3076923076923\n",
      "\n",
      "Dimensions:\t8\n",
      "At least one:\t47.69230769230769\n",
      "All:\t\t92.3076923076923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check dimensions 1 to 8 (smallest category (2) has 8 concepts/dimensions)\n",
    "for dim in range(8):\n",
    "    rec = []\n",
    "\n",
    "    for i,(target,entry) in enumerate(zip(targets,entries)):\n",
    "\n",
    "        parameters = \"nlp_resources/nlp_\"+str(i+1)+\".npz\"\n",
    "        tasks_json = \"tasks/tasks_\"+str(i+1)+\".json\"\n",
    "\n",
    "        # Verify the targets\n",
    "        #with open(tasks_json, 'r') as fp:\n",
    "        #    tasks_dict = json.load(fp)\n",
    "        #tasks = list(tasks_dict.values())\n",
    "        #for t in target:\n",
    "        #    print(tasks[t])\n",
    "        #print('')\n",
    "\n",
    "        recs = nlp(entry,parameters,tasks_json,dim)\n",
    "        rec_str = list(recs.keys())[0:5]\n",
    "        rec.append([int(r) for r in rec_str])\n",
    "    \n",
    "    preds = np.asarray(rec)\n",
    "\n",
    "    acc = np.asarray([[True if p in target else False for p in pred] for pred,target in zip(preds,targets)])\n",
    "\n",
    "    print('Dimensions:\\t{0}\\nAt least one:\\t{1}\\nAll:\\t\\t{2}\\n'.format(\n",
    "        dim+1,\n",
    "        100*(sum((acc).flatten())/(targets.shape[0]*targets.shape[1])),\n",
    "        100*(sum(np.sum(acc, axis=1) > 0)/targets.shape[0])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
